#ivar github: https://github.com/andersen-lab/ivar
#ivar manual: https://andersen-lab.github.io/ivar/html/manualpage.html#autotoc_md17

#titus variant calling with minimap: https://github.com/clmattson/dib_rotation/blob/master/doc/02_conda.md

#processed data (batch 1): https://ucdavis.app.box.com/file/802677521499

```{bash}
#in crick:
 srun -J ivar -t 6:00:00 --mem=14gb -c 1 --pty bash
#make empty conda env
conda create -y -n ivar
conda install ivar

#Titus's workflow requires mamba
#in (base) conda:
conda install mamba

#from github:
git clone https://github.com/ucd-covid/minimap-workflow.git
cd minimap-workflow/
mamba env create --name covid -f environment.yml
conda activate covid

#get example data 
cmatt5@crick:~/SARS-COV2/minimap-workflow/data/PRJNA661613$ wget https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR12596165/6_09_N1_S30_R1_001.fastq.gz.1
cmatt5@crick:~/SARS-COV2/minimap-workflow/data/PRJNA661613$ wget https://sra-pub-sars-cov2.s3.amazonaws.com/sra-src/SRR12596165/6_09_N1_S30_R2_001.fastq.gz.1

#get UCDavis environmental data
#in theoory you can copy straight from bioshare to crick but I downloaded to local and then scp'ed to crick. 
#put under /data 
```

**TO RUN TITUS' MINIMAP PIPELINE**
**Workflow details and variant calling parameters**

rule map_reads maps reads with minimap.
rule rmdup removes perfectly duplicated reads with samtools rmdup.
the rule call_variants_spec does highly specific variant calling with bcftools
QUAL<40 eliminates low quality mappings
DP<10 eliminates
GT!="1/1" eliminates heterozygous calls
the rule consensus_from_vcf builds a consensus using the VCF output by call_variants_spec, with bcftools consensus
the rule mask_low_coverage takes the consensus produced by consensus_from_vcf and masks bases that have mapping coverage strictly below 5

```{bash}
srun -J covid -t 6:00:00 --mem=25gb -c 1 --pty bash
conda activate covid

**#FIRST: need to get interleaved reads with 'abundtrime' name**
#do this by running ```Snakefile.merge-pe``` in the directory with the raw data (aka /data for now)
#This currently makes interleaved files with abundtrim.fq.gz in the name. Something like:
#snakemake -s /path/to/Snakefile.merge-pe -j 4
(covid) cmatt5@c2-3:~/SARS-COV2/minimap-workflow/data$ snakemake -s ~/SARS-COV2/minimap-workflow/Snakefile.merge-pe -j 4
#found 1 samples
#Building DAG of jobs...
#Using shell: /bin/bash
#Provided cores: 4
#Rules claiming more threads will be scaled down.
#Job counts:
#        count   jobs
#        1       all
#        1       merge_pe
#        2
#Select jobs to execute...

#[Wed Jun 16 11:59:52 2021]
#rule merge_pe:
#    input: GSR_SWIFT_2021_04_06_O20_001_020821_C1E1_S14_R1_001.fastq.gz, GSR_SWIFT_2021_04_06_O20_001_020821_C1E1_S14_R2_001.fastq.gz
#    output: GSR_SWIFT_2021_04_06_O20_001_020821_C1E1_S14.abundtrim.fq.gz
#    jobid: 1
#    wildcards: sample=GSR_SWIFT_2021_04_06_O20_001_020821_C1E1_S14

#[Wed Jun 16 12:03:32 2021]
#Finished job 1.
#1 of 2 steps (50%) done
#Select jobs to execute...

#[Wed Jun 16 12:03:32 2021]
#localrule all:
#    input: GSR_SWIFT_2021_04_06_O20_001_020821_C1E1_S14.abundtrim.fq.gz
#    jobid: 0

#[Wed Jun 16 12:03:32 2021]
#Finished job 0.
#2 of 2 steps (100%) done
#Complete log: /home/cmatt5/SARS-COV2/minimap-workflow/data/.snakemake/log/2021-06-16T115951.950604.snakemake.log

#file called ```GSR_SWIFT_2021_04_06_O20_001_020821_C1E1_S14.abundtrim.fq.gz``` now in ./data

#now to run workflow:
#in minimap-workflow (i think?)
snakemake --use-conda -j 4 

#error:  File "/home/cmatt5/SARS-COV2/minimap-workflow/./variants_of_interest.py", line 3, in <module>
#    import screed
#ModuleNotFoundError: No module named 'screed'

 Probably have to conda install screed, whatever that is
 
#delete all output to start over
snakemake --delete-all-output --cores 1
#try again:
snakemake --use-conda -j 4 
#success!

```

OK. now i have some results, but the output .vcf file in unfiltered.









##### get OSU pipeline going #####

# workflow outline frpom Brett Tyler LAb: 

"## Trim reads w/ bbduk
bbduk.sh ktrim=r k=23 mink=11 hdist=1 qtrim=r1 trimq=15 minlength=30 maxns=0 {params.mem} t={threads} in1={input.R1} in2={input.R2} out1={output.R1} out2={output.R2} ref=/local/cluster/bbmap/resources/adapters.fa
## Run BWA mem
bwa mem -t {threads} -M -R '{params.rg}' {input.fa} {input.fq} > {output.sam}
## Coord sort sams
samtools sort -n {input.sam} -o {output.sam}
## Soft Clip
primerclip {input.master} {input.sam} {output.sam}
## Convert sam to bam
samtools view -b {input.sam} -o {output.bam}
## Sort reads
samtools sort {input.bam} -o {output.sort}
## Index reads
samtools index {input.bam}
## Get depth from Bam files
samtools depth -a {input.bam} | awk '$3 < 5' | cut -f 1,2 > {output.depth}
## call variants in g.vcf mode
gatk HaplotypeCaller --java-options {params.mem} -R {input.ref} -ERC GVCF -stand-call-conf 20 --dont-use-soft-clipped-bases -mbq 20 --max-reads-per-alignment-start 0 --linked-de-bruijn-graph --recover-all-dangling-branches --sample-ploidy {params.ploidy} --input {input.dedup} --output {output.gvcf}
## Combine gvcfs
gatk CombineGVCFs --java-options {params.mem} -R {input.ref} {lgvcfs} -O {output.gvcf}
## Genotype merged gvcf with gatk GenotypeGVCFs
gatk GenotypeGVCFs --java-options {params.mem} {knvars} -R {input.ref} -V {input.gvcf} -O {output.raw_vcf}
## Custom R script"

``` {bash}


#add agbiome conda channel 
cmatt5@crick:~$ srun -J osu -t 6:00:00 --mem=25gb -c 1 --pty bash
(base) cmatt5@c2-3:~$ conda config --add channels agbiome
(base) cmatt5@c2-3:~$ conda activate osu
(osu) cmatt5@c2-3:~$ conda install -c agbiome bbtools

#instal bwa
git clone https://github.com/lh3/bwa.git
cd bwa; make
./bwa mem ref.fa read-se.fq.gz | gzip -3 > aln-se.sam.gz
./bwa mem ref.fa read1.fq read2.fq | gzip -3 > aln-pe.sam.gzgcc -c -g -Wall -Wno-unused-function -O2 -DHAVE_PTHREAD -DUSE_MALLOC_WRAPPERS  utils.c -o utils.o

#samtools
 conda install samtools
 # All requested packages already installed.
 
#install primerclip
conda install -c bioconda primerclip
#install gatk
conda install -c bioconda gatk

#download wuhan hu 1 reference .fa from ncbi

#copy example files fromm minimap workflo dir

#start workflow on example data
bbduk.sh ktrim=r k=23 mink=11 hdist=1 qtrim=r1 trimq=15 minlength=30 maxns=0 {params.mem} t={threads} in1={input.R1} in2={input.R2} out1={output.R1} out2={output.R2} ref=/local/cluster/bbmap/resources/adapters.fa

```
